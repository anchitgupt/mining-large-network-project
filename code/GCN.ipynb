{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, graclus\n",
    "from torch_cluster import graclus_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2011raw = pd.read_csv('E:\\Github\\MLN\\FinalData/2011raw.csv')\n",
    "dfgraph = pd.read_csv('E:\\Github\\MLN\\FinalData/GraphProp.csv')\n",
    "dfcensus = pd.read_csv('E:\\Github\\MLN\\FinalData/census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2011raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2011raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.LongTensor(list(range(0,1223)))\n",
    "weight = torch.tensor(df2011raw['Total migrants Persons'].tolist())\n",
    "num_nodes = df2011raw.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgraph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfcensus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    print('{:28}  {}'.format(dfcensus['State'].tolist()[i], dfgraph.Id.tolist()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgraph['Pop Size'] = dfcensus['Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgraph = dfgraph.drop(columns=['Label', 'timeset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures = dfgraph[['weighted indegree', 'weighted outdegree','indegree', 'outdegree', 'Degree', 'pageranks','clustering', 'Pop Size']]\n",
    "dffeatures[['Area','Density','DecadalGrowth']] = dfcensus[['Area','Density','DecadalGrowth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decadalgrowth = []\n",
    "\n",
    "for i in dffeatures['DecadalGrowth'].str[:-1].tolist():\n",
    "    try:\n",
    "        decadalgrowth.append(np.float(i))\n",
    "    except:\n",
    "        decadalgrowth.append(-1 * float(i[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures['DecadalGrowth'] = decadalgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures['Area']  =  pd.to_numeric(dffeatures['Area'].str.replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures['Density']  =  pd.to_numeric(dffeatures['Density'].str.replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "dffeatures[dffeatures.columns] = scaler.fit_transform(dffeatures[dffeatures.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures = dffeatures.drop(columns=['indegree', 'outdegree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures = dffeatures.drop(columns=['Degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures = dffeatures.drop(columns=['clustering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures = dffeatures.drop(columns=['weighted indegree', 'weighted outdegree', 'pageranks',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dffeatures.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "a = dfcensus['Literacyrate'].to_numpy()\n",
    "sorted(Counter(np.floor(a/10).astype(int)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.floor(a/20).astype(int)\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('E:/Github/MLN/FinalData/Top10InterState.csv')\n",
    "set(t['Target'].unique()) - set(t['Source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming ={}\n",
    "for i in range(32):\n",
    "    naming[t['Source'].unique()[i]] = i\n",
    "naming['goa'] =32\n",
    "naming['lakshadweep'] = 33\n",
    "naming['sikkim']=34\n",
    "for i in range(t.shape[0]):\n",
    "    t['Source'].iloc[i] = naming[t['Source'].iloc[i]]\n",
    "    t['Target'].iloc[i] = naming[t['Target'].iloc[i]]\n",
    "a = t['Source']\n",
    "b = t['Target']\n",
    "g = nx.DiGraph()\n",
    "lp2011=[]\n",
    "for i,j,k in zip(a, b, t['Weight'].tolist()):\n",
    "    if i!=j:\n",
    "        lp2011.append((i,j,k))\n",
    "g.add_weighted_edges_from(lp2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from dgl import DGLGraph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.DGLGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message phase: all nodes first compute and send out messages along out-going edges.\n",
    "mp = fn.copy_u('h', 'm')\n",
    "# reduce phase: all node then collect in-coming messages, aggregate them and update their own embedding.\n",
    "rp = fn.sum('m', 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        # linear layer\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "   \n",
    "    def forward(self, g, inputs):\n",
    "        # g is the graph and the inputs is the input node features\n",
    "        # first perform linear transformation\n",
    "        h = self.linear(inputs)\n",
    "        # set the node features\n",
    "        g.ndata['h'] = h\n",
    "        # using DGL api\n",
    "        # 1st arg:\n",
    "        # message phase: all nodes first compute and send out messages along out-going edges.\n",
    "        # 2nd arg:\n",
    "        # reduce phase: all node then collect in-coming messages, aggregate them and update their own embedding.\n",
    "        g.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h'))\n",
    "        # get the result node features\n",
    "        h = g.ndata.pop('h')\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        # Custom Linear Layer 1\n",
    "        self.gcn1 = GraphConv(len(dffeatures.columns), 4)\n",
    "        # Custom Linear Layer 2 \n",
    "        self.gcn2 = GraphConv(4, 2)\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        x = th.relu(self.gcn1(g, features))\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the GCN\n",
    "net = GCN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = {\n",
    "#     0:4,\n",
    "#     1:3,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt2 = []\n",
    "for i in gt:\n",
    "    if i == 4:\n",
    "        gt2.append(0)\n",
    "    else:\n",
    "        gt2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs   =  torch.from_numpy(dffeatures.values).float()\n",
    "labels   = th.tensor([0, 1])  \n",
    "labelled = th.tensor([0, 33]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = []\n",
    "# for i in range(len(dffeatures.values)):\n",
    "#     if i == 14 or i==33:\n",
    "#         pass\n",
    "#     else:\n",
    "#         d.append(dffeatures.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs   =  torch.from_numpy(np.array(d)).float()\n",
    "# labels   = th.tensor([0, 1])  \n",
    "# labelled = th.tensor([0, 32]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store NN o/p\n",
    "scores = []\n",
    "net = GCN()\n",
    "# list to store time\n",
    "t=[]\n",
    "# optimizer\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-1)\n",
    "# loop through epocs\n",
    "for epoch in range(1000):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "        \n",
    "    logits = net(G, inputs)\n",
    "    scores.append(logits)\n",
    "    \n",
    "    logp = F.log_softmax(logits, 1)\n",
    "\n",
    "    # computing loss for the given nodes\n",
    "    loss = F.nll_loss(logp[labelled], labels)\n",
    "    \n",
    "    # taking max score from each node prediction \n",
    "    d, indices = th.max(logits, dim=1)\n",
    "    print(indices)\n",
    "    # checking against the ground truth\n",
    "    acc=th.sum(th.IntTensor(gt2) == indices).float()/len(gt2)\n",
    "    # backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch >=3:\n",
    "        t.append(time.time() - t0)\n",
    "#     if epoch %100 ==0:    \n",
    "    print('Epoch %d | Loss: %.4f | Accuracy: %.4f | Time(s) %.4f'% (epoch, loss.item(), acc, np.mean(t)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2011raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = df2011raw.groupby('Area Name')['Work/employment Persons'].apply(lambda grp: grp.nlargest(10)).index\n",
    "top5data = []\n",
    "for i in top5:\n",
    "    top5data.append([df2011raw['Last residence'].loc[i[1]], i[0], df2011raw['Work/employment Persons'].loc[i[1]]])\n",
    "t = pd.DataFrame(top5data, columns=['Source', 'Target','Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(t['Target'].unique()) - set(t['Source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming ={}\n",
    "for i in range(31):\n",
    "    naming[t['Source'].unique()[i]] = i\n",
    "naming['andaman & nicobar islands'] =31\n",
    "naming['goa'] =32\n",
    "naming['lakshadweep'] = 33\n",
    "naming['sikkim']=34\n",
    "for i in range(t.shape[0]):\n",
    "    t['Source'].iloc[i] = naming[t['Source'].iloc[i]]\n",
    "    t['Target'].iloc[i] = naming[t['Target'].iloc[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t['Source']\n",
    "b = t['Target']\n",
    "g = nx.DiGraph()\n",
    "lp2011=[]\n",
    "for i,j,k in zip(a, b, t['Weight'].tolist()):\n",
    "    if i!=j:\n",
    "        lp2011.append((i,j,k))\n",
    "g.add_weighted_edges_from(lp2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.DGLGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store NN o/p\n",
    "scores = []\n",
    "net = GCN()\n",
    "# list to store time\n",
    "t=[]\n",
    "# optimizer\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-1)\n",
    "# loop through epocs\n",
    "for epoch in range(100):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "        \n",
    "    logits = net(G, inputs)\n",
    "    scores.append(logits)\n",
    "    \n",
    "    logp = F.log_softmax(logits, 1)\n",
    "\n",
    "    # computing loss for the given nodes\n",
    "    loss = F.nll_loss(logp[labelled], labels)\n",
    "    \n",
    "    # taking max score from each node prediction \n",
    "    d, indices = th.max(logits, dim=1)\n",
    "    print(indices)\n",
    "    # checking against the ground truth\n",
    "    acc=th.sum(th.IntTensor(gt2) == indices).float()/len(gt2)\n",
    "    # backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch >=3:\n",
    "        t.append(time.time() - t0)\n",
    "#     if epoch %100 ==0:    \n",
    "    print('Epoch %d | Loss: %.4f | Accuracy: %.4f | Time(s) %.4f'% (epoch, loss.item(), acc, np.mean(t)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2011raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = df2011raw.groupby('Area Name')['Business Persons'].apply(lambda grp: grp.nlargest(10)).index\n",
    "top5data = []\n",
    "for i in top5:\n",
    "    top5data.append([df2011raw['Last residence'].loc[i[1]], i[0], df2011raw['Business Persons'].loc[i[1]]])\n",
    "t = pd.DataFrame(top5data, columns=['Source', 'Target','Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(t['Target'].unique()) - set(t['Source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t['Source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming ={}\n",
    "for i in range(28):\n",
    "    naming[t['Source'].unique()[i]] = i\n",
    "naming['andaman & nicobar islands'] =28\n",
    "naming['arunachal pradesh'] =29\n",
    "naming['dadra & nagar haveli'] =30\n",
    "naming['daman & diu'] =31\n",
    "naming['goa'] =32\n",
    "naming['lakshadweep'] = 33\n",
    "naming['sikkim']=34\n",
    "for i in range(t.shape[0]):\n",
    "    t['Source'].iloc[i] = naming[t['Source'].iloc[i]]\n",
    "    t['Target'].iloc[i] = naming[t['Target'].iloc[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t['Source']\n",
    "b = t['Target']\n",
    "g = nx.DiGraph()\n",
    "lp2011=[]\n",
    "for i,j,k in zip(a, b, t['Weight'].tolist()):\n",
    "    if i!=j:\n",
    "        lp2011.append((i,j,k))\n",
    "g.add_weighted_edges_from(lp2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.DGLGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store NN o/p\n",
    "scores = []\n",
    "net = GCN()\n",
    "# list to store time\n",
    "t=[]\n",
    "# optimizer\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-1)\n",
    "# loop through epocs\n",
    "for epoch in range(100):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "        \n",
    "    logits = net(G, inputs)\n",
    "    scores.append(logits)\n",
    "    \n",
    "    logp = F.log_softmax(logits, 1)\n",
    "\n",
    "    # computing loss for the given nodes\n",
    "    loss = F.nll_loss(logp[labelled], labels)\n",
    "    \n",
    "    # taking max score from each node prediction \n",
    "    d, indices = th.max(logits, dim=1)\n",
    "    print(indices)\n",
    "    # checking against the ground truth\n",
    "    acc=th.sum(th.IntTensor(gt2) == indices).float()/len(gt2)\n",
    "    # backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch >=3:\n",
    "        t.append(time.time() - t0)\n",
    "#     if epoch %100 ==0:    \n",
    "    print('Epoch %d | Loss: %.4f | Accuracy: %.4f | Time(s) %.4f'% (epoch, loss.item(), acc, np.mean(t)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankgdp = pd.read_excel('E:\\Github\\MLN\\FinalData\\GDP2001.xlsx', sheet_name='Sheet2', header=None, names=['2001', '2011'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankgdp = rankgdp.drop(columns=['2001'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = rankgdp['2011'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankgdp[rankgdp['2011'] > 100000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankgdp[rankgdp['2011'] <= 100000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt3 =[]\n",
    "for i in gt:\n",
    "    if gt.index(i) in rankgdp[rankgdp['2011'] > 100000].index:\n",
    "        gt3.append(1)\n",
    "    else:\n",
    "        gt3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(gt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffeatures.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitf0c61f1e05474c16bf06d0152d368821"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}